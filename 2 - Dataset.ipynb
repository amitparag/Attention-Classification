{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ed09af-1062-4712-9e42-5566dd210869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "import av\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from vit_pytorch.vivit import ViT\n",
    "torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33194a2a-5531-409b-8fb5-8aa747c19b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd10ee7c-5fb5-4296-a3e0-e9fffc6ee747",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc73a4f-f73d-42d1-a14c-ef381bcc5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67bf5753-e1b3-43d3-91c6-d3e175d48c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets():\n",
    "\n",
    "    \n",
    "    root_dir = './dataset'\n",
    "    source_slip_videos = './dataset/slip'  \n",
    "    source_wriggle_videos = './dataset/wriggle' \n",
    "    \n",
    "    classes = ['slip', 'wriggle']\n",
    "    subsets = ['train', 'validation', 'test']\n",
    "    split_ratios = [0.7, 0.15, 0.15]  # Train, validation, test split ratios\n",
    "    \n",
    "    # Create root directory\n",
    "    if not os.path.exists(root_dir):\n",
    "        os.makedirs(root_dir)\n",
    "    \n",
    "    # Create subsets directories and copy videos\n",
    "    for subset in subsets:\n",
    "        subset_dir = os.path.join(root_dir, subset)\n",
    "        if not os.path.exists(subset_dir):\n",
    "            os.makedirs(subset_dir)\n",
    "    \n",
    "        for class_name in classes:\n",
    "            class_dir = os.path.join(subset_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                os.makedirs(class_dir)\n",
    "            \n",
    "            source_videos = source_slip_videos if class_name == 'slip' else source_wriggle_videos\n",
    "            video_files = [f for f in os.listdir(source_videos) if f.endswith('.avi')]\n",
    "            random.shuffle(video_files)  # Shuffle video files\n",
    "            \n",
    "            split_ratio = split_ratios[subsets.index(subset)]\n",
    "            num_videos = int(len(video_files) * split_ratio)\n",
    "            \n",
    "            for video_file in video_files[:num_videos]:\n",
    "                src_path = os.path.join(source_videos, video_file)\n",
    "                dest_path = os.path.join(class_dir, video_file)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "    \n",
    "    print(\"Directory structure and video copying completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "593ead1f-11a8-4f7e-a37c-17cf478e7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c8134ce-3049-4fc1-b37c-467f55dfd51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.videos = self._load_videos()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _load_videos(self):\n",
    "        videos = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            for video_file in os.listdir(class_dir):\n",
    "                if video_file.endswith('.avi'):\n",
    "                    video_path = os.path.join(class_dir, video_file)\n",
    "                    videos.append((video_path, self.class_to_idx[class_name]))\n",
    "        return videos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.videos[idx]\n",
    "\n",
    "        video = imageio.get_reader(video_path, 'ffmpeg')  # Open video with imageio\n",
    "        \n",
    "        #print(\"Video:\", video_path)\n",
    "        #print(\"Number of frames:\", len(video))\n",
    "        #print(\"Height:\", video.get_meta_data()[\"size\"][1])\n",
    "        #print(\"Width:\", video.get_meta_data()[\"size\"][0])\n",
    "        \n",
    "        \n",
    "        frames = []\n",
    "        \n",
    "        for frame in video:\n",
    "            frame = frame[:, :, :3]  # Keep only the first three channels (RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "        video.close()\n",
    "        \n",
    "        #container = av.open(video_path)  # Open the video file with pyav\n",
    "        #frames = []\n",
    "        #for frame in container.decode(video=0):  # Loop through video frames\n",
    "        #    img = frame.to_image()\n",
    "        #    img = img.convert('RGB')  # Convert to RGB format\n",
    "        #    frame_array = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(img.size[1], img.size[0], 3)\n",
    "        #    frames.append(frame_array.numpy())\n",
    "        \n",
    "        #container.close() \n",
    "        \n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "            video_tensor = torch.stack(frames)\n",
    "        #print(video_tensor.shape)\n",
    "        return video_tensor.permute(1, 0, 2, 3), label  # Permute to (batch, channels, frames, height, width)\n",
    "\n",
    "data_transform = Compose([\n",
    "    ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078f1a46-2922-4f01-8a38-0c290f40d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './dataset'\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "val_dir = os.path.join(root_dir, 'validation')\n",
    "test_dir = os.path.join(root_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65a7814-748b-4cd3-9f45-87883c04bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(train_dir, transform=data_transform)\n",
    "val_dataset = VideoDataset(val_dir, transform=data_transform)\n",
    "test_dataset = VideoDataset(test_dir, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96acecb9-7ceb-4df0-af6d-254ab5497d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0,shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3e3fe5c-89a2-4f5f-8096-2289da9be828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage of the DataLoader\n",
    "#for videos, labels in train_loader:\n",
    "    # videos will have the shape: (batch, channels, frames, height, width)\n",
    "#    print(\"Video batch shape:\", videos.shape)\n",
    "#    print(\"Label batch:\", labels)\n",
    "#    break  # Stop after the first batch for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421dddbb-ae57-470e-b5a8-2f38938ade44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slip', 'wriggle']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "889040bc-b4f2-426e-9ddd-dca1403fa2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ViT(\n",
    "    image_size = (240,320),          # image size\n",
    "    frames = 450,                    # number of frames\n",
    "    image_patch_size = (24,32),      # image patch size\n",
    "    frame_patch_size = 45,           # frame patch size\n",
    "    num_classes = 2,\n",
    "    dim = 728,\n",
    "    spatial_depth = 4,               # depth of the spatial transformer\n",
    "    temporal_depth = 4,              # depth of the temporal transformer\n",
    "    heads = 4,\n",
    "    mlp_dim = 1024\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc72418-5299-4c90-a35a-81cbbb7455bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7304e9a-04e0-4cba-bd4c-3c7dfbf66283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, loss function, and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69ac1050-c97c-434f-ac73-c174268ed523",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/VideoVision/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/VideoVision/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for videos, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2c8f8-2163-4e30-8938-50d0197e9686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
